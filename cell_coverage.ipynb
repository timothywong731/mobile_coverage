{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd91c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "from dateutil import rrule\n",
    "from shapely.geometry import LineString, Polygon, MultiPolygon, Point, MultiPoint\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely.ops import polygonize, unary_union\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from typing import Any, Dict\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e395ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = datetime(2025, 3, 1)\n",
    "END_DATE = datetime(2025, 6, 30)\n",
    "\n",
    "# Iterate through every month from START_DATE to END_DATE\n",
    "all_months = list(rrule.rrule(rrule.MONTHLY, dtstart=START_DATE, until=END_DATE))\n",
    "\n",
    "# Format as ISO dates\n",
    "all_months = [x.strftime(\"%Y-%m-%d\") for x in all_months]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af9846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "ds = load_dataset(\"joefee/cell-service-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eefce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "df = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb1918c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unique_cell</th>\n",
       "      <th>measurement_type_name</th>\n",
       "      <th>in_outdoor_state</th>\n",
       "      <th>value</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>signal_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-01 15:48:22.455</td>\n",
       "      <td>d66c4660d6f77433b503d2e0159ce7053bdd76dc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>52.941137</td>\n",
       "      <td>-1.180972</td>\n",
       "      <td>-112.207920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-01 03:15:47.436</td>\n",
       "      <td>bb8abb3cfc1d2eb47edba5f216a0078d829575f2</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Indoor</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>52.756933</td>\n",
       "      <td>-1.517308</td>\n",
       "      <td>-120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-01 12:26:07.000</td>\n",
       "      <td>3fb387a3995398fc5602cc0e431ce935044c594e</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Outdoor</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>52.770351</td>\n",
       "      <td>-1.208205</td>\n",
       "      <td>-108.719592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-01 09:49:03.000</td>\n",
       "      <td>e662f0bf857466dafbb8da554abfec2e86335deb</td>\n",
       "      <td>None</td>\n",
       "      <td>Surely Outdoor</td>\n",
       "      <td>46.8</td>\n",
       "      <td>52.644803</td>\n",
       "      <td>-1.189687</td>\n",
       "      <td>-103.070615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-01 00:56:44.711</td>\n",
       "      <td>0fed16bc81fbd0ef53e6071cf9fd8449d810fdfc</td>\n",
       "      <td>None</td>\n",
       "      <td>Surely Indoor</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>52.887214</td>\n",
       "      <td>-1.534917</td>\n",
       "      <td>-96.205530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504611</th>\n",
       "      <td>2025-06-30 09:21:01.666</td>\n",
       "      <td>5a6685a51c70f12cc7b03db00a18db684e60835c</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Indoor</td>\n",
       "      <td>1.4</td>\n",
       "      <td>52.641313</td>\n",
       "      <td>-1.095236</td>\n",
       "      <td>-86.808643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504612</th>\n",
       "      <td>2025-06-30 06:38:45.485</td>\n",
       "      <td>71f3829f43969786e0adb953a0390146e67d1b3b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>52.771422</td>\n",
       "      <td>-1.197493</td>\n",
       "      <td>-102.676974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504613</th>\n",
       "      <td>2025-06-30 15:24:45.000</td>\n",
       "      <td>334868f2087c2f62b8cf9d658ba7ffec4bf9ca77</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Outdoor</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>52.645504</td>\n",
       "      <td>-1.126884</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504614</th>\n",
       "      <td>2025-06-30 01:56:10.540</td>\n",
       "      <td>0f96898a83142aa9cd14d8f27dfa1aa529af2fca</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Indoor</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>52.627893</td>\n",
       "      <td>-1.336682</td>\n",
       "      <td>-104.649658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504615</th>\n",
       "      <td>2025-06-30 22:09:47.990</td>\n",
       "      <td>c132096ae6e1c62bf1b24eae9759b502fa51103c</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Indoor</td>\n",
       "      <td>0.5</td>\n",
       "      <td>52.662074</td>\n",
       "      <td>-1.067613</td>\n",
       "      <td>-117.042570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31504616 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp                               unique_cell  \\\n",
       "0         2025-03-01 15:48:22.455  d66c4660d6f77433b503d2e0159ce7053bdd76dc   \n",
       "1         2025-03-01 03:15:47.436  bb8abb3cfc1d2eb47edba5f216a0078d829575f2   \n",
       "2         2025-03-01 12:26:07.000  3fb387a3995398fc5602cc0e431ce935044c594e   \n",
       "3         2025-03-01 09:49:03.000  e662f0bf857466dafbb8da554abfec2e86335deb   \n",
       "4         2025-03-01 00:56:44.711  0fed16bc81fbd0ef53e6071cf9fd8449d810fdfc   \n",
       "...                           ...                                       ...   \n",
       "31504611  2025-06-30 09:21:01.666  5a6685a51c70f12cc7b03db00a18db684e60835c   \n",
       "31504612  2025-06-30 06:38:45.485  71f3829f43969786e0adb953a0390146e67d1b3b   \n",
       "31504613  2025-06-30 15:24:45.000  334868f2087c2f62b8cf9d658ba7ffec4bf9ca77   \n",
       "31504614  2025-06-30 01:56:10.540  0f96898a83142aa9cd14d8f27dfa1aa529af2fca   \n",
       "31504615  2025-06-30 22:09:47.990  c132096ae6e1c62bf1b24eae9759b502fa51103c   \n",
       "\n",
       "         measurement_type_name  in_outdoor_state  value   latitude  longitude  \\\n",
       "0                         None              None  -91.0  52.941137  -1.180972   \n",
       "1                         None   Probably Indoor   -9.0  52.756933  -1.517308   \n",
       "2                         None  Probably Outdoor -108.0  52.770351  -1.208205   \n",
       "3                         None    Surely Outdoor   46.8  52.644803  -1.189687   \n",
       "4                         None     Surely Indoor   -7.0  52.887214  -1.534917   \n",
       "...                        ...               ...    ...        ...        ...   \n",
       "31504611                  None   Probably Indoor    1.4  52.641313  -1.095236   \n",
       "31504612                  None              None -109.0  52.771422  -1.197493   \n",
       "31504613                  None  Probably Outdoor -105.0  52.645504  -1.126884   \n",
       "31504614                  None   Probably Indoor  -14.0  52.627893  -1.336682   \n",
       "31504615                  None   Probably Indoor    0.5  52.662074  -1.067613   \n",
       "\n",
       "          signal_level  \n",
       "0          -112.207920  \n",
       "1          -120.000000  \n",
       "2          -108.719592  \n",
       "3          -103.070615  \n",
       "4           -96.205530  \n",
       "...                ...  \n",
       "31504611    -86.808643  \n",
       "31504612   -102.676974  \n",
       "31504613           NaN  \n",
       "31504614   -104.649658  \n",
       "31504615   -117.042570  \n",
       "\n",
       "[31504616 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Browse the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9e6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old dataset, unused\n",
    "# df = pd.read_csv(\"hf://datasets/joefee/service-data/np_obs_jit_jf_tf_tw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9a9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin this into signal level categories\n",
    "# UK Ofcom Reference URL: \n",
    "# https://www.ofcom.org.uk/siteassets/resources/documents/phones-telecoms-and-internet/comparing-service-quality/2025/map-your-mobile-2025-threshold-methodology.pdf\n",
    "df[\"signal_level_category\"] = pd.cut(df[\"signal_level\"], bins=[-np.inf, -105, -95, -82, -74, np.inf], labels=[\"1. Very Weak\", \"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a41192b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify cells with sufficient data points\n",
    "# Cells must also have signal_level_category value\n",
    "min_points_required = 30\n",
    "\n",
    "sufficient_data_cells = duckdb.query(f\"\"\"\n",
    "WITH monthly_count AS (\n",
    "    SELECT\n",
    "        unique_cell, \n",
    "        date_trunc('month', CAST(timestamp AS timestamp)) as month, \n",
    "        COUNT(*) as count\n",
    "    FROM df\n",
    "    WHERE signal_level IS NOT NULL\n",
    "    GROUP BY unique_cell, month HAVING COUNT(*) >= {30}\n",
    ")\n",
    "SELECT unique_cell \n",
    "FROM monthly_count \n",
    "GROUP BY unique_cell\n",
    "HAVING COUNT(DISTINCT month) = {len(all_months)}\n",
    "\"\"\").to_df()['unique_cell'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0c38a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells with sufficient data: 5244\n",
      "Total number of cells: 36631\n"
     ]
    }
   ],
   "source": [
    "# Identify all unique cells\n",
    "# remove outlier cells\n",
    "outlier_cells = [\"4dc7c9ec434ed06502767136789763ec11d2c4b7\"]\n",
    "sufficient_data_cells = [cell for cell in sufficient_data_cells if cell not in outlier_cells]\n",
    "\n",
    "print(f\"Number of cells with sufficient data: {len(sufficient_data_cells)}\")\n",
    "print(f\"Total number of cells: {df['unique_cell'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d5e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_svm_boundary_geom(df, **args) -> BaseGeometry:\n",
    "    \"\"\"\n",
    "    Generate a valid MultiPolygon with true cut-out holes from One-Class SVM boundary.\n",
    "    Converts input longitude/latitude columns to float type if they are not already.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'longitude' and 'latitude' columns.\n",
    "                        These columns can contain numbers or Decimal objects.\n",
    "        **args: Keyword arguments passed directly to svm.OneClassSVM.\n",
    "\n",
    "    Returns:\n",
    "        BaseGeometry: A valid MultiPolygon geometry representing the SVM boundary,\n",
    "                      or None if the boundary could not be created.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['longitude', 'latitude']):\n",
    "        print(\"Error: Input df must be a pandas DataFrame with 'longitude' and 'latitude' columns.\")\n",
    "        return None\n",
    "\n",
    "    if len(df) < 2:\n",
    "        print(\"Warning: Need at least 2 data points for SVM.\")\n",
    "        return None\n",
    "\n",
    "    # --- Convert coordinate columns to float type ---\n",
    "    # This resolves the Decimal vs float TypeError\n",
    "    try:\n",
    "        df_copy = df.copy() # Work on a copy to avoid modifying the original DataFrame\n",
    "        df_copy['longitude'] = df_copy['longitude'].astype(float)\n",
    "        df_copy['latitude'] = df_copy['latitude'].astype(float)\n",
    "        coords = df_copy[['longitude', 'latitude']].values\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"Error converting coordinate columns to float: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 1. Train the SVM ---\n",
    "    try:\n",
    "        clf = svm.OneClassSVM(**args)\n",
    "        clf.fit(coords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during SVM training: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. Create mesh grid for contouring ---\n",
    "    # Now calculations will use standard floats\n",
    "    x_min, x_max = coords[:, 0].min(), coords[:, 0].max()\n",
    "    y_min, y_max = coords[:, 1].min(), coords[:, 1].max()\n",
    "\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_margin = x_range * 1 if x_range > 1e-9 else 0.1\n",
    "    y_margin = y_range * 1 if y_range > 1e-9 else 0.1\n",
    "\n",
    "    x_min -= x_margin\n",
    "    x_max += x_margin\n",
    "    y_min -= y_margin\n",
    "    y_max += y_margin\n",
    "\n",
    "    resolution = 500\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n",
    "                        np.linspace(y_min, y_max, resolution))\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    try:\n",
    "        Z = clf.decision_function(grid_points).reshape(xx.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during SVM decision function evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. Extract contour lines at level 0 (the boundary) ---\n",
    "    fig, ax = plt.subplots()\n",
    "    try:\n",
    "        cs = ax.contour(xx, yy, Z, levels=[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during contour generation: {e}\")\n",
    "        plt.close(fig)\n",
    "        return None\n",
    "    plt.close(fig)\n",
    "\n",
    "    if not cs.allsegs or not cs.allsegs[0]:\n",
    "        print(\"Warning: No contour lines found at level 0.\")\n",
    "        return None\n",
    "\n",
    "    segments = cs.allsegs[0]\n",
    "    lines = [LineString(seg) for seg in segments if len(seg) >= 2]\n",
    "\n",
    "    if not lines:\n",
    "        print(\"Warning: No valid LineStrings created from contour segments.\")\n",
    "        return None\n",
    "\n",
    "    # --- 4. Polygonize the lines ---\n",
    "    try:\n",
    "        all_polygons = list(polygonize(lines))\n",
    "    except Exception as e:\n",
    "        print(f\"Error during polygonization: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not all_polygons:\n",
    "        print(\"Warning: Polygonization did not yield any polygons.\")\n",
    "        return None\n",
    "\n",
    "    # --- 5. Classify polygons and perform unary union ---\n",
    "    positive_polygons = []\n",
    "    for p in all_polygons:\n",
    "        if p.is_valid and p.area > 1e-9:\n",
    "            rep_point = p.representative_point()\n",
    "            try:\n",
    "                decision_val = clf.decision_function([[rep_point.x, rep_point.y]])[0]\n",
    "                if decision_val >= 0:\n",
    "                    # Ensure the polygon added is valid - unary_union can struggle with invalid inputs\n",
    "                    if p.is_valid:\n",
    "                        positive_polygons.append(p)\n",
    "                    else:\n",
    "                    # Attempt to buffer by 0 to fix potential self-intersections\n",
    "                        buffered_p = p.buffer(0)\n",
    "                        if buffered_p.is_valid and isinstance(buffered_p, Polygon):\n",
    "                                positive_polygons.append(buffered_p)\n",
    "                        else:\n",
    "                            print(f\"Warning: Skipping invalid polygon generated during classification step even after buffer(0). Area: {p.area}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error checking decision function for a polygon point: {e}\")\n",
    "\n",
    "\n",
    "    if not positive_polygons:\n",
    "        print(\"Warning: No valid polygons were classified as inside the SVM boundary.\")\n",
    "        return None\n",
    "\n",
    "    # --- 6. Unary Union to merge positive polygons and create holes ---\n",
    "    try:\n",
    "        # Filter again for validity just before union, as buffer(0) might create MultiPolygons\n",
    "        valid_positive_polygons = [poly for poly in positive_polygons if poly.is_valid and isinstance(poly, Polygon)]\n",
    "        if not valid_positive_polygons:\n",
    "            print(\"Warning: No valid polygons remaining before unary union.\")\n",
    "            return None\n",
    "        result_geom = unary_union(valid_positive_polygons)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch potential errors during unary_union (often related to complex topology)\n",
    "        print(f\"Error during unary union: {e}\")\n",
    "        # As a fallback, try creating a MultiPolygon directly from the valid positive polygons\n",
    "        # This might result in overlaps instead of proper union, but is better than nothing.\n",
    "        print(\"Attempting fallback: creating MultiPolygon from individual positive polygons.\")\n",
    "        try:\n",
    "            result_geom = MultiPolygon(valid_positive_polygons)\n",
    "            if not result_geom.is_valid:\n",
    "                print(\"Warning: Fallback MultiPolygon is invalid.\")\n",
    "                # Try buffer(0) on the multipolygon as a last resort\n",
    "                buffered_result = result_geom.buffer(0)\n",
    "                if buffered_result.is_valid:\n",
    "                    print(\"Fallback MultiPolygon fixed with buffer(0).\")\n",
    "                    result_geom = buffered_result\n",
    "                else:\n",
    "                    print(\"Error: Fallback MultiPolygon remains invalid even after buffer(0). Cannot proceed.\")\n",
    "                    return None\n",
    "        except Exception as fallback_e:\n",
    "            print(f\"Error during fallback MultiPolygon creation: {fallback_e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # --- 7. Format output as MultiPolygon GeoJSON mapping ---\n",
    "    final_multi_poly = None\n",
    "    if result_geom is None: # Should not happen with current logic, but check anyway\n",
    "        print(\"Error: Resulting geometry is None after union/fallback.\")\n",
    "        return None\n",
    "\n",
    "    # Simplify handling by ensuring result_geom is always iterable (list of polygons)\n",
    "    geoms_to_wrap = []\n",
    "    if isinstance(result_geom, Polygon):\n",
    "        if result_geom.is_valid:\n",
    "            geoms_to_wrap = [result_geom]\n",
    "    elif isinstance(result_geom, MultiPolygon):\n",
    "        # Filter out invalid geoms within the MultiPolygon if any\n",
    "        geoms_to_wrap = [g for g in result_geom.geoms if g.is_valid and isinstance(g, Polygon)]\n",
    "    elif hasattr(result_geom, 'geoms'): # Handle GeometryCollection\n",
    "        print(f\"Warning: unary_union resulted in a GeometryCollection. Filtering for valid Polygons.\")\n",
    "        geoms_to_wrap = [g for g in result_geom.geoms if g.is_valid and isinstance(g, Polygon)]\n",
    "\n",
    "    if not geoms_to_wrap:\n",
    "        print(\"Warning: No valid polygons found in the final geometry after union/cleanup.\")\n",
    "        return None\n",
    "\n",
    "    # Create the final MultiPolygon\n",
    "    final_multi_poly = MultiPolygon(geoms_to_wrap)\n",
    "\n",
    "    # Final validity check\n",
    "    if final_multi_poly.is_valid:\n",
    "        return final_multi_poly\n",
    "    else:\n",
    "        # Try one last buffer(0) fix\n",
    "        print(\"Warning: Final MultiPolygon is invalid. Attempting buffer(0) fix.\")\n",
    "        buffered_final = final_multi_poly.buffer(0)\n",
    "        if buffered_final.is_valid and isinstance(buffered_final, (Polygon, MultiPolygon)):\n",
    "            # Re-wrap if buffer resulted in a single Polygon\n",
    "            if isinstance(buffered_final, Polygon):\n",
    "                final_multi_poly = MultiPolygon([buffered_final])\n",
    "            else:\n",
    "                final_multi_poly = buffered_final\n",
    "            print(\"Final MultiPolygon fixed with buffer(0).\")\n",
    "            return final_multi_poly\n",
    "        else:\n",
    "            print(\"Error: Final MultiPolygon remains invalid even after buffer(0).\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0cea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convex_hull_geom(df, quantile:float=0.95) -> BaseGeometry:\n",
    "    \"\"\"\n",
    "    Generate a valid convex hull MultiPolygon from input DataFrame.\n",
    "    Converts input longitude/latitude columns to float type if they are not already.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'longitude' and 'latitude' columns.\n",
    "        **args: Currently unused, present for API consistency.\n",
    "    Returns:\n",
    "        BaseGeometry: Shapely geometry object representing the convex hull, or None if unsuccessful.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct a convex hull with shapely using train_df\n",
    "    points_train = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "\n",
    "    # Find center of mass among these points\n",
    "    multipoint = MultiPoint(points_train)\n",
    "    center_of_mass = multipoint.centroid\n",
    "\n",
    "    # For each point, calculate distance to center of mass\n",
    "    distances = [point.distance(center_of_mass) for point in points_train]\n",
    "    \n",
    "    # Find the 95% percentile distance\n",
    "    threshold_distance = pd.Series(distances).quantile(quantile)\n",
    "\n",
    "    # Filter points to only those within the threshold distance\n",
    "    filtered_points = [point for point, distance in zip(points_train, distances) if distance <= threshold_distance]\n",
    "\n",
    "    # Create new multipoint from filtered points\n",
    "    multipoint_filtered = MultiPoint(filtered_points)\n",
    "    \n",
    "    # Calculate the convex hull\n",
    "    convex_hull = multipoint_filtered.convex_hull\n",
    "\n",
    "    if convex_hull.is_valid:\n",
    "        return convex_hull\n",
    "    else:\n",
    "        print(\"Warning: Convex hull is invalid.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa904057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_metric(y_true, y_pred, metric:str):\n",
    "    \"\"\"\n",
    "    Compute a specific classification metric based on the provided metric name.\n",
    "    Handles zero division by returning 0 for precision, recall, and F1 score in such cases.\n",
    "\n",
    "    Args:\n",
    "        y_true (list): True binary labels.\n",
    "        y_pred (list): Predicted binary labels.\n",
    "        metric (str): Metric to compute - one of 'accuracy', 'precision', 'recall', 'f1'.\n",
    "    \n",
    "    Returns:\n",
    "        float: The computed metric score.\n",
    "    \"\"\"\n",
    "    metric = metric.lower()\n",
    "    if metric == 'accuracy':\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif metric == 'precision':\n",
    "        return precision_score(y_true, y_pred, zero_division=0)\n",
    "    elif metric == 'recall':\n",
    "        return recall_score(y_true, y_pred, zero_division=0)\n",
    "    elif metric == 'f1':\n",
    "        return f1_score(y_true, y_pred, zero_division=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported metric: {metric}. Choose from 'accuracy', 'precision', 'recall', 'f1'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b381efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowed_cells = [\"5a6685a51c70f12cc7b03db00a18db684e60835c\"]\n",
    "\n",
    "# Take the first 50 cells for processing\n",
    "allowed_cells = sufficient_data_cells[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9189b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cell(cell_id:str, metric:str='f1'):\n",
    "    \"\"\"\n",
    "    Process a single cell to generate coverage boundaries using Convex Hull and SVM methods.\n",
    "    Evaluates the boundaries using a specified classification metric and saves results to GeoJSON and CSV files\n",
    "\n",
    "    Args:\n",
    "        cell_id (str): The unique identifier of the cell to process.\n",
    "    \"\"\"\n",
    "    \n",
    "    my_geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "    }\n",
    "\n",
    "    my_best_convex_hull:Dict[Any, tuple[float, BaseGeometry]] = {}\n",
    "    my_best_svm_hyperplanes:Dict[Any, tuple[float, BaseGeometry]] = {}\n",
    "\n",
    "    # Select data for this cell only\n",
    "    df_cell = df[df['unique_cell'] == cell_id]\n",
    "    print(f\"---\")\n",
    "    print(f\"Processing cell {cell_id}\")\n",
    "\n",
    "    # Exclude last month\n",
    "    for month in all_months[:-1]:\n",
    "        \n",
    "        # Identify train/test split\n",
    "        # Use one month for training, next month for testing\n",
    "        df_train = df_cell[(pd.to_datetime(df_cell['timestamp']).dt.to_period('M') == pd.to_datetime(month).to_period('M'))]\n",
    "        test_month = (pd.to_datetime(month).to_period('M') + 1).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # For df_test, use all points regardless of cells.\n",
    "        # Subset +- longitude/latitude range of training data by 20%\n",
    "        # Crop to a bounding box to avoid huge area\n",
    "        lon_min, lon_max = df_train['longitude'].min(), df_train['longitude'].max()\n",
    "        lat_min, lat_max = df_train['latitude'].min(), df_train['latitude'].max()\n",
    "\n",
    "        lon_margin = (lon_max - lon_min) * 0.2 if (lon_max - lon_min) > 1e-9 else 0.01\n",
    "        lat_margin = (lat_max - lat_min) * 0.2 if (lat_max - lat_min) > 1e-9 else 0.01\n",
    "        lon_min -= lon_margin\n",
    "        lon_max += lon_margin\n",
    "        lat_min -= lat_margin\n",
    "        lat_max += lat_margin\n",
    "\n",
    "        # Subset the test month and all data within the expanded bounding box\n",
    "        df_test = df[(pd.to_datetime(df['timestamp']).dt.to_period('M') == pd.to_datetime(test_month).to_period('M')) &\n",
    "                      (df['longitude'] >= lon_min) & (df['longitude'] <= lon_max) &\n",
    "                      (df['latitude'] >= lat_min) & (df['latitude'] <= lat_max)]\n",
    "        \n",
    "        # Sample down test set if too large\n",
    "        # Cap data points at 50% for testing to speed up evaluation\n",
    "        n_test = int(len(df_test) * 0.5)\n",
    "        if len(df_test) > n_test:\n",
    "            df_test = df_test.sample(n=n_test).reset_index(drop=True)\n",
    "\n",
    "        print(f\"---Training month: {month}, Testing month: {test_month}---\")\n",
    "        print(f\"Train: {len(df_train)}, Test: {len(df_test)}\")\n",
    "\n",
    "        # OC-SVM arguments\n",
    "        svm_args = {\n",
    "            'kernel': 'rbf',\n",
    "            'nu': float(0.08),  # Lower value allow more flexible boundary\n",
    "            'gamma': float(len(df_train) * 30)  # Higher value allow more complex decision boundary\n",
    "        }\n",
    "\n",
    "        # Convex hull arguments\n",
    "        convex_hull_args = {\n",
    "            'quantile': 0.98\n",
    "        }\n",
    "\n",
    "        # Run boundary generation for different signal level categories\n",
    "        # Starting from very weak to very strong\n",
    "        # Cumulative union of all these geoms to ensure coverage of all signal levels\n",
    "        levels = {\n",
    "            5: [\"5. Very Strong\"],\n",
    "            4: [\"4. Strong\", \"5. Very Strong\"],\n",
    "            3: [\"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "            2: [\"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "            1: [\"1. Very Weak\", \"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "        }\n",
    "        \n",
    "        # Generate SVM boundaries for each level\n",
    "        for key, level in levels.items():\n",
    "            df_subset_level_train = df_train[df_train[\"signal_level_category\"].isin(level)]\n",
    "            print(f\"Generating SVM for cell {cell_id} at levels {level} with {len(df_subset_level_train)} training points\")\n",
    "\n",
    "            # Use all levels for testing\n",
    "            df_subset_level_test = df_test\n",
    "\n",
    "            if len(df_subset_level_train) < min_points_required:\n",
    "                print(f\"Skipping SVM for cell {cell_id} at levels {level} due to insufficient points ({len(df_subset_level_train)} < {min_points_required})\")\n",
    "                continue\n",
    "            \n",
    "            # Convert to shapely Points\n",
    "            points_level_train = [Point(xy) for xy in zip(df_subset_level_train['longitude'], df_subset_level_train['latitude'])]\n",
    "            points_level_test = [Point(xy) for xy in zip(df_subset_level_test['longitude'], df_subset_level_test['latitude'])]\n",
    "            \n",
    "            # Generate convex hull for this level\n",
    "            convex_hull = generate_convex_hull_geom(df_subset_level_train, **convex_hull_args)\n",
    "\n",
    "            # Run SVM boundary generation\n",
    "            buffer = 0.000 \n",
    "            # svm_hyperplane = generate_svm_boundary_geom(df_subset_level_train, **svm_args)\n",
    "\n",
    "            # Perform grid search over SVM hyperparameters to find best model\n",
    "            svm_args = [\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.02), \"gamma\": float(3.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.02), \"gamma\": float(2.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.02), \"gamma\": float(1.0e4)},\n",
    "\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.04), \"gamma\": float(3.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.04), \"gamma\": float(2.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.04), \"gamma\": float(1.0e4)},\n",
    "\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.06), \"gamma\": float(3.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.06), \"gamma\": float(2.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.06), \"gamma\": float(1.0e4)},\n",
    "            ]\n",
    "            \n",
    "            def replace_if_better(\n",
    "                metric_model:tuple[float, BaseGeometry], \n",
    "                current_metric_value:float, \n",
    "                geometry:BaseGeometry, \n",
    "                metric:str,\n",
    "                model_name:str,\n",
    "            ) -> tuple[float, BaseGeometry]:\n",
    "                \"\"\"\n",
    "                Replace the metric model if the current metric value is better.\n",
    "                \"\"\"\n",
    "\n",
    "                if metric_model is None:\n",
    "                    print(f\"Best {model_name} model found for metric '{metric}': {current_metric_value}\")\n",
    "                    return (current_metric_value, geometry)\n",
    "                else:\n",
    "                    if current_metric_value > metric_model[0]:\n",
    "                        print(f\"Better {model_name} model found for metric '{metric}': {metric_model[0]} -> {current_metric_value}\")\n",
    "                        return (current_metric_value, geometry)\n",
    "                    \n",
    "                    return metric_model\n",
    "            \n",
    "            # Calculate true labels for test set\n",
    "            # It is true if the point is in any of the levels being considered and belonging to the same unique_cell\n",
    "            # Note: y_true is the same for both models since test set is the same\n",
    "            y_true = [row.signal_level_category in level and row.unique_cell==cell_id for row in df_subset_level_test.itertuples()]\n",
    "            # y_true = [x in level for x in df_subset_level_test[\"signal_level_category\"].tolist()]\n",
    "\n",
    "            # Grid search over SVM hyperparameters\n",
    "            svm_hyperplane = None\n",
    "            best_svm_metric = -1.0\n",
    "\n",
    "            for args in svm_args:\n",
    "\n",
    "                # Using SVM args\n",
    "                print(f\"Trying SVM args: {args}\")\n",
    "                svm_hyperplane = generate_svm_boundary_geom(df_subset_level_train, **args)\n",
    "\n",
    "                if svm_hyperplane is not None:\n",
    "                    # Add buffer radius to have wider coverage\n",
    "                    if buffer > 0:\n",
    "                        try:\n",
    "                            print(f\"Applying buffer of {buffer} degrees to SVM geometry\")\n",
    "                            svm_hyperplane = svm_hyperplane.buffer(buffer)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: buffering geometry failed for cell {cell_id}, levels {level}: {e}\")\n",
    "\n",
    "                    # Construct confusion matrix values\n",
    "                    y_pred_svm = [True if svm_hyperplane.contains(point) else False for point in points_level_test]\n",
    "\n",
    "                    # Calculate metrics for SVM method\n",
    "                    metric_svm = get_classification_metric(y_true, y_pred_svm, metric)\n",
    "\n",
    "                    # Store the best SVM model\n",
    "                    my_best_svm_hyperplanes[key] = replace_if_better(my_best_svm_hyperplanes.get(key, None), metric_svm, svm_hyperplane, metric, \"svm\")\n",
    "\n",
    "            # Calculate y_pred for convex hull\n",
    "            y_pred_chull = [True if convex_hull.contains(point) else False for point in points_level_test]\n",
    "\n",
    "            # Calculate metrics for convex hull method\n",
    "            my_metric_chull = get_classification_metric(y_true, y_pred_chull, metric)\n",
    "\n",
    "            # Store the best convex hull model\n",
    "            my_best_convex_hull[key] = replace_if_better(my_best_convex_hull.get(key, None), my_metric_chull, convex_hull, metric, \"convex hull\")\n",
    "\n",
    "\n",
    "    def ensure_overlapping_polygons(data:Dict[Any,tuple[float, BaseGeometry]]) -> Dict[Any,tuple[float, BaseGeometry]]:\n",
    "        \"\"\"\n",
    "        Ensure that successive geometries in the input dictionary are fully overlapping.\n",
    "        For example, level 5 should be fully within level 4, and level 4 within level 3, etc.\n",
    "        This means level 4 is the union of level 4 and level 5, level 3 is the union of level 3, level 4 and level 5, etc.\n",
    "\n",
    "        Args:\n",
    "            data (Dict[Any, tuple[float, BaseGeometry]]): Dictionary with keys and (metric, geometry) tuples.\n",
    "        Returns:\n",
    "            Dict[Any, tuple[float, BaseGeometry]]: Filtered dictionary with only overlapping polygons.\n",
    "        \"\"\"\n",
    "        \n",
    "        sorted_keys = sorted(data.keys(), reverse=True)  # Start from highest level\n",
    "        accumulated_geom = None\n",
    "        filtered_data = {}\n",
    "\n",
    "        for key in sorted_keys:\n",
    "            metric, geom = data[key]\n",
    "            if geom is None or not geom.is_valid:\n",
    "                print(f\"Warning: Skipping invalid or None geometry for level {key}.\")\n",
    "                continue\n",
    "\n",
    "            if accumulated_geom is None:\n",
    "                accumulated_geom = geom\n",
    "            else:\n",
    "                try:\n",
    "                    accumulated_geom = accumulated_geom.union(geom)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Union operation failed for level {key}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            filtered_data[key] = (metric, accumulated_geom)\n",
    "\n",
    "        # Reverse back to original order\n",
    "        return dict(sorted(filtered_data.items()))\n",
    "\n",
    "    # Ensure overlapping polygons for both models\n",
    "    my_best_convex_hull = ensure_overlapping_polygons(my_best_convex_hull)\n",
    "    my_best_svm_hyperplanes = ensure_overlapping_polygons(my_best_svm_hyperplanes)\n",
    "\n",
    "    # Compare the metrics of these two methods\n",
    "    metrics_chull = [(k, my_best_convex_hull[k][0]) for k in my_best_convex_hull.keys()]\n",
    "    metrics_svm = [(k, my_best_svm_hyperplanes[k][0]) for k in my_best_svm_hyperplanes.keys()]\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_chull, columns=['level', 'chull']).merge(\n",
    "        pd.DataFrame(metrics_svm, columns=['level', 'svm']), on='level', how='outer'\n",
    "    )\n",
    "\n",
    "    # Add unique_cell column\n",
    "    metrics_df[\"unique_cell\"] = cell_id\n",
    "\n",
    "    # Export this cell's metrics to CSV\n",
    "    os.makedirs('cells', exist_ok=True)\n",
    "    metrics_df.to_csv(f\"cells/cell_{cell_id}_metrics.csv\", index=False)\n",
    "\n",
    "\n",
    "    # Append the best convex hulls to list\n",
    "    for key, value in my_best_convex_hull.items():\n",
    "        my_geojson['features'].append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"cell_id\": cell_id,\n",
    "                \"levels\": key,\n",
    "                \"model\": \"convex_hull\",\n",
    "                \"fill\": \"#1100FF\",\n",
    "            },\n",
    "            \"geometry\": shapely.geometry.mapping(value[1]),\n",
    "        })\n",
    "\n",
    "    # Append the best SVM hyperplanes to list\n",
    "    for key, value in my_best_svm_hyperplanes.items():\n",
    "        my_geojson['features'].append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"cell_id\": cell_id,\n",
    "                \"levels\": key,\n",
    "                \"model\": \"svm\",\n",
    "                \"fill\": \"#FF0000\",\n",
    "            },\n",
    "            \"geometry\": shapely.geometry.mapping(value[1]),\n",
    "        })\n",
    "        \n",
    "    # Export geojson for this cell\n",
    "    import os\n",
    "    os.makedirs('cells', exist_ok=True)\n",
    "    with open(f\"cells/cell_{cell_id}.geojson\", \"w\") as f:\n",
    "        json.dump(my_geojson, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f08b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1105: UserWarning: On Windows, max_workers cannot exceed 61 due to limitations of the operating system.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 490, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Timothy Wong\\AppData\\Local\\Temp\\ipykernel_47532\\2198713432.py\", line 169, in process_cell\nNameError: name 'metric' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_cell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msufficient_data_cells\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mallowed_cells\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Timothy Wong\\Repositories\\mobile_coverage\\.venv\\Lib\\site-packages\\joblib\\parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'metric' is not defined"
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=100)(delayed(process_cell)(x) for x in sufficient_data_cells if x in allowed_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebad8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"f1\"  # Options: \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "\n",
    "for cell_id in [x for x in sufficient_data_cells if x in allowed_cells]:\n",
    "\n",
    "    my_geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "    }\n",
    "\n",
    "    my_best_convex_hull:Dict[Any, tuple[float, BaseGeometry]] = {}\n",
    "    my_best_svm_hyperplanes:Dict[Any, tuple[float, BaseGeometry]] = {}\n",
    "\n",
    "    # Select data for this cell only\n",
    "    df_cell = df[df['unique_cell'] == cell_id]\n",
    "    print(f\"---\")\n",
    "    print(f\"Processing cell {cell_id}\")\n",
    "\n",
    "    # Exclude last month\n",
    "    for month in all_months[:-1]:\n",
    "        \n",
    "        # Identify train/test split\n",
    "        # Use one month for training, next month for testing\n",
    "        df_train = df_cell[(pd.to_datetime(df_cell['timestamp']).dt.to_period('M') == pd.to_datetime(month).to_period('M'))]\n",
    "        test_month = (pd.to_datetime(month).to_period('M') + 1).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # For df_test, use all points regardless of cells.\n",
    "        # Subset +- longitude/latitude range of training data by 20%\n",
    "        # Crop to a bounding box to avoid huge area\n",
    "        lon_min, lon_max = df_train['longitude'].min(), df_train['longitude'].max()\n",
    "        lat_min, lat_max = df_train['latitude'].min(), df_train['latitude'].max()\n",
    "\n",
    "        lon_margin = (lon_max - lon_min) * 0.2 if (lon_max - lon_min) > 1e-9 else 0.01\n",
    "        lat_margin = (lat_max - lat_min) * 0.2 if (lat_max - lat_min) > 1e-9 else 0.01\n",
    "        lon_min -= lon_margin\n",
    "        lon_max += lon_margin\n",
    "        lat_min -= lat_margin\n",
    "        lat_max += lat_margin\n",
    "\n",
    "        # Subset the test month and all data within the expanded bounding box\n",
    "        df_test = df[(pd.to_datetime(df['timestamp']).dt.to_period('M') == pd.to_datetime(test_month).to_period('M')) &\n",
    "                      (df['longitude'] >= lon_min) & (df['longitude'] <= lon_max) &\n",
    "                      (df['latitude'] >= lat_min) & (df['latitude'] <= lat_max)]\n",
    "        \n",
    "        # Sample down test set if too large\n",
    "        # Cap data points at 50% for testing to speed up evaluation\n",
    "        n_test = int(len(df_test) * 0.5)\n",
    "        if len(df_test) > n_test:\n",
    "            df_test = df_test.sample(n=n_test).reset_index(drop=True)\n",
    "\n",
    "        print(f\"---Training month: {month}, Testing month: {test_month}---\")\n",
    "        print(f\"Train: {len(df_train)}, Test: {len(df_test)}\")\n",
    "\n",
    "        # OC-SVM arguments\n",
    "        svm_args = {\n",
    "            'kernel': 'rbf',\n",
    "            'nu': float(0.08),  # Lower value allow more flexible boundary\n",
    "            'gamma': float(len(df_train) * 30)  # Higher value allow more complex decision boundary\n",
    "        }\n",
    "\n",
    "        # Convex hull arguments\n",
    "        convex_hull_args = {\n",
    "            'quantile': 0.98\n",
    "        }\n",
    "\n",
    "        # Run boundary generation for different signal level categories\n",
    "        # Starting from very weak to very strong\n",
    "        # Cumulative union of all these geoms to ensure coverage of all signal levels\n",
    "        levels = {\n",
    "            5: [\"5. Very Strong\"],\n",
    "            4: [\"4. Strong\", \"5. Very Strong\"],\n",
    "            3: [\"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "            2: [\"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "            1: [\"1. Very Weak\", \"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "        }\n",
    "        \n",
    "        # Generate SVM boundaries for each level\n",
    "        for key, level in levels.items():\n",
    "            df_subset_level_train = df_train[df_train[\"signal_level_category\"].isin(level)]\n",
    "            print(f\"Generating SVM for cell {cell_id} at levels {level} with {len(df_subset_level_train)} training points\")\n",
    "\n",
    "            # Use all levels for testing\n",
    "            df_subset_level_test = df_test\n",
    "\n",
    "            if len(df_subset_level_train) < min_points_required:\n",
    "                print(f\"Skipping SVM for cell {cell_id} at levels {level} due to insufficient points ({len(df_subset_level_train)} < {min_points_required})\")\n",
    "                continue\n",
    "            \n",
    "            # Convert to shapely Points\n",
    "            points_level_train = [Point(xy) for xy in zip(df_subset_level_train['longitude'], df_subset_level_train['latitude'])]\n",
    "            points_level_test = [Point(xy) for xy in zip(df_subset_level_test['longitude'], df_subset_level_test['latitude'])]\n",
    "            \n",
    "            # Generate convex hull for this level\n",
    "            convex_hull = generate_convex_hull_geom(df_subset_level_train, **convex_hull_args)\n",
    "\n",
    "            # Run SVM boundary generation\n",
    "            buffer = 0.000 \n",
    "            # svm_hyperplane = generate_svm_boundary_geom(df_subset_level_train, **svm_args)\n",
    "\n",
    "            # Perform grid search over SVM hyperparameters to find best model\n",
    "            svm_args = [\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.02), \"gamma\": float(3.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.02), \"gamma\": float(2.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.02), \"gamma\": float(1.0e4)},\n",
    "\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.04), \"gamma\": float(3.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.04), \"gamma\": float(2.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.04), \"gamma\": float(1.0e4)},\n",
    "\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.06), \"gamma\": float(3.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.06), \"gamma\": float(2.0e4)},\n",
    "                {\"kernel\": \"rbf\", \"nu\": float(0.06), \"gamma\": float(1.0e4)},\n",
    "            ]\n",
    "            \n",
    "            def replace_if_better(\n",
    "                metric_model:tuple[float, BaseGeometry], \n",
    "                current_metric_value:float, \n",
    "                geometry:BaseGeometry, \n",
    "                metric:str,\n",
    "                model_name:str,\n",
    "            ) -> tuple[float, BaseGeometry]:\n",
    "                \"\"\"\n",
    "                Replace the metric model if the current metric value is better.\n",
    "                \"\"\"\n",
    "\n",
    "                if metric_model is None:\n",
    "                    print(f\"Best {model_name} model found for metric '{metric}': {current_metric_value}\")\n",
    "                    return (current_metric_value, geometry)\n",
    "                else:\n",
    "                    if current_metric_value > metric_model[0]:\n",
    "                        print(f\"Better {model_name} model found for metric '{metric}': {metric_model[0]} -> {current_metric_value}\")\n",
    "                        return (current_metric_value, geometry)\n",
    "                    \n",
    "                    return metric_model\n",
    "            \n",
    "            # Calculate true labels for test set\n",
    "            # It is true if the point is in any of the levels being considered and belonging to the same unique_cell\n",
    "            # Note: y_true is the same for both models since test set is the same\n",
    "            y_true = [row.signal_level_category in level and row.unique_cell==cell_id for row in df_subset_level_test.itertuples()]\n",
    "            # y_true = [x in level for x in df_subset_level_test[\"signal_level_category\"].tolist()]\n",
    "\n",
    "            # Grid search over SVM hyperparameters\n",
    "            svm_hyperplane = None\n",
    "            best_svm_metric = -1.0\n",
    "\n",
    "            for args in svm_args:\n",
    "\n",
    "                # Using SVM args\n",
    "                print(f\"Trying SVM args: {args}\")\n",
    "                svm_hyperplane = generate_svm_boundary_geom(df_subset_level_train, **args)\n",
    "\n",
    "                if svm_hyperplane is not None:\n",
    "                    # Add buffer radius to have wider coverage\n",
    "                    if buffer > 0:\n",
    "                        try:\n",
    "                            print(f\"Applying buffer of {buffer} degrees to SVM geometry\")\n",
    "                            svm_hyperplane = svm_hyperplane.buffer(buffer)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: buffering geometry failed for cell {cell_id}, levels {level}: {e}\")\n",
    "\n",
    "                    # Construct confusion matrix values\n",
    "                    y_pred_svm = [True if svm_hyperplane.contains(point) else False for point in points_level_test]\n",
    "\n",
    "                    # Calculate metrics for SVM method\n",
    "                    metric_svm = get_classification_metric(y_true, y_pred_svm, metric)\n",
    "\n",
    "                    # Store the best SVM model\n",
    "                    my_best_svm_hyperplanes[key] = replace_if_better(my_best_svm_hyperplanes.get(key, None), metric_svm, svm_hyperplane, metric, \"svm\")\n",
    "\n",
    "            # Calculate y_pred for convex hull\n",
    "            y_pred_chull = [True if convex_hull.contains(point) else False for point in points_level_test]\n",
    "\n",
    "            # Calculate metrics for convex hull method\n",
    "            my_metric_chull = get_classification_metric(y_true, y_pred_chull, metric)\n",
    "\n",
    "            # Store the best convex hull model\n",
    "            my_best_convex_hull[key] = replace_if_better(my_best_convex_hull.get(key, None), my_metric_chull, convex_hull, metric, \"convex hull\")\n",
    "\n",
    "\n",
    "    def ensure_overlapping_polygons(data:Dict[Any,tuple[float, BaseGeometry]]) -> Dict[Any,tuple[float, BaseGeometry]]:\n",
    "        \"\"\"\n",
    "        Ensure that successive geometries in the input dictionary are fully overlapping.\n",
    "        For example, level 5 should be fully within level 4, and level 4 within level 3, etc.\n",
    "        This means level 4 is the union of level 4 and level 5, level 3 is the union of level 3, level 4 and level 5, etc.\n",
    "\n",
    "        Args:\n",
    "            data (Dict[Any, tuple[float, BaseGeometry]]): Dictionary with keys and (metric, geometry) tuples.\n",
    "        Returns:\n",
    "            Dict[Any, tuple[float, BaseGeometry]]: Filtered dictionary with only overlapping polygons.\n",
    "        \"\"\"\n",
    "        \n",
    "        sorted_keys = sorted(data.keys(), reverse=True)  # Start from highest level\n",
    "        accumulated_geom = None\n",
    "        filtered_data = {}\n",
    "\n",
    "        for key in sorted_keys:\n",
    "            metric, geom = data[key]\n",
    "            if geom is None or not geom.is_valid:\n",
    "                print(f\"Warning: Skipping invalid or None geometry for level {key}.\")\n",
    "                continue\n",
    "\n",
    "            if accumulated_geom is None:\n",
    "                accumulated_geom = geom\n",
    "            else:\n",
    "                try:\n",
    "                    accumulated_geom = accumulated_geom.union(geom)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Union operation failed for level {key}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            filtered_data[key] = (metric, accumulated_geom)\n",
    "\n",
    "        # Reverse back to original order\n",
    "        return dict(sorted(filtered_data.items()))\n",
    "\n",
    "    # Ensure overlapping polygons for both models\n",
    "    my_best_convex_hull = ensure_overlapping_polygons(my_best_convex_hull)\n",
    "    my_best_svm_hyperplanes = ensure_overlapping_polygons(my_best_svm_hyperplanes)\n",
    "\n",
    "    # Compare the metrics of these two methods\n",
    "    metrics_chull = [(k, my_best_convex_hull[k][0]) for k in my_best_convex_hull.keys()]\n",
    "    metrics_svm = [(k, my_best_svm_hyperplanes[k][0]) for k in my_best_svm_hyperplanes.keys()]\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_chull, columns=['level', 'chull']).merge(\n",
    "        pd.DataFrame(metrics_svm, columns=['level', 'svm']), on='level', how='outer'\n",
    "    )\n",
    "\n",
    "    # Add unique_cell column\n",
    "    metrics_df[\"unique_cell\"] = cell_id\n",
    "\n",
    "    # Merge this cell metrics into a single dataframe for all cells\n",
    "    if 'all_metrics' in locals():\n",
    "        all_metrics = pd.concat([all_metrics, metrics_df], ignore_index=True)\n",
    "    else:\n",
    "        all_metrics = pd.concat([metrics_df], ignore_index=True)\n",
    "\n",
    "    # Append the best convex hulls to list\n",
    "    for key, value in my_best_convex_hull.items():\n",
    "        my_geojson['features'].append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"cell_id\": cell_id,\n",
    "                \"levels\": key,\n",
    "                \"model\": \"convex_hull\",\n",
    "                \"fill\": \"#1100FF\",\n",
    "            },\n",
    "            \"geometry\": shapely.geometry.mapping(value[1]),\n",
    "        })\n",
    "\n",
    "    # Append the best SVM hyperplanes to list\n",
    "    for key, value in my_best_svm_hyperplanes.items():\n",
    "        my_geojson['features'].append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"cell_id\": cell_id,\n",
    "                \"levels\": key,\n",
    "                \"model\": \"svm\",\n",
    "                \"fill\": \"#FF0000\",\n",
    "            },\n",
    "            \"geometry\": shapely.geometry.mapping(value[1]),\n",
    "        })\n",
    "        \n",
    "    # Export geojson for this cell\n",
    "    import os\n",
    "    os.makedirs('cells', exist_ok=True)\n",
    "    with open(f\"cells/cell_{cell_id}.geojson\", \"w\") as f:\n",
    "        json.dump(my_geojson, f, indent=2)\n",
    "\n",
    "# Export all_metrics to CSV\n",
    "all_metrics.to_csv(\"cells/metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot metrics for convex hull vs SVM at different levels\n",
    "# Use bar plot with error bars to compare the metrics side by side\n",
    "\n",
    "# Calculate the error bars (standard deviation) for each method at each level\n",
    "metrics_summary = all_metrics.groupby('level').agg({\n",
    "    'chull': ['mean', 'std'],\n",
    "    'svm': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "metrics_summary.columns = ['level', 'chull_mean', 'chull_std', 'svm_mean', 'svm_std']\n",
    "\n",
    "# Plotting (bars side by side with error bars)\n",
    "# Use log scale\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(metrics_summary['level'].astype(str).astype(float) - 0.2, metrics_summary['chull_mean'], yerr=metrics_summary['chull_std'], width=0.4, label='Convex Hull', alpha=0.7, capsize=5)\n",
    "plt.bar(metrics_summary['level'].astype(str).astype(float) + 0.2, metrics_summary['svm_mean'], yerr=metrics_summary['svm_std'], width=0.4, label='SVM', alpha=0.7, capsize=5)\n",
    "plt.xlabel('Signal Strength')\n",
    "plt.ylabel(f'Mean {metric.upper()} Score')\n",
    "plt.title(f'Comparison of Convex Hull vs SVM by {metric.upper()} Score')\n",
    "plt.xticks(metrics_summary['level'].astype(str).astype(float), metrics_summary['level'].astype(str))\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile-coverage-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
