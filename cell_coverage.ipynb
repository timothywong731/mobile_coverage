{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd91c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from typing import List\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "import duckdb\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Optional\n",
    "from shapely.geometry import shape\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e395ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import rrule\n",
    "\n",
    "START_DATE = datetime(2025, 3, 1)\n",
    "END_DATE = datetime(2025, 6, 30)\n",
    "\n",
    "# Iterate through every month from START_DATE to END_DATE\n",
    "all_months = list(rrule.rrule(rrule.MONTHLY, dtstart=START_DATE, until=END_DATE))\n",
    "\n",
    "# Format as ISO dates\n",
    "all_months = [x.strftime(\"%Y-%m-%d\") for x in all_months]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af9846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "ds = load_dataset(\"joefee/cell-service-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eefce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "df = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb1918c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unique_cell</th>\n",
       "      <th>measurement_type_name</th>\n",
       "      <th>in_outdoor_state</th>\n",
       "      <th>value</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>signal_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-01 15:48:22.455</td>\n",
       "      <td>d66c4660d6f77433b503d2e0159ce7053bdd76dc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>52.941137</td>\n",
       "      <td>-1.180972</td>\n",
       "      <td>-112.207920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-01 03:15:47.436</td>\n",
       "      <td>bb8abb3cfc1d2eb47edba5f216a0078d829575f2</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Indoor</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>52.756933</td>\n",
       "      <td>-1.517308</td>\n",
       "      <td>-120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-01 12:26:07.000</td>\n",
       "      <td>3fb387a3995398fc5602cc0e431ce935044c594e</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Outdoor</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>52.770351</td>\n",
       "      <td>-1.208205</td>\n",
       "      <td>-108.719592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-01 09:49:03.000</td>\n",
       "      <td>e662f0bf857466dafbb8da554abfec2e86335deb</td>\n",
       "      <td>None</td>\n",
       "      <td>Surely Outdoor</td>\n",
       "      <td>46.8</td>\n",
       "      <td>52.644803</td>\n",
       "      <td>-1.189687</td>\n",
       "      <td>-103.070615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-01 00:56:44.711</td>\n",
       "      <td>0fed16bc81fbd0ef53e6071cf9fd8449d810fdfc</td>\n",
       "      <td>None</td>\n",
       "      <td>Surely Indoor</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>52.887214</td>\n",
       "      <td>-1.534917</td>\n",
       "      <td>-96.205530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504611</th>\n",
       "      <td>2025-06-30 09:21:01.666</td>\n",
       "      <td>5a6685a51c70f12cc7b03db00a18db684e60835c</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Indoor</td>\n",
       "      <td>1.4</td>\n",
       "      <td>52.641313</td>\n",
       "      <td>-1.095236</td>\n",
       "      <td>-86.808643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504612</th>\n",
       "      <td>2025-06-30 06:38:45.485</td>\n",
       "      <td>71f3829f43969786e0adb953a0390146e67d1b3b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>52.771422</td>\n",
       "      <td>-1.197493</td>\n",
       "      <td>-102.676974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504613</th>\n",
       "      <td>2025-06-30 15:24:45.000</td>\n",
       "      <td>334868f2087c2f62b8cf9d658ba7ffec4bf9ca77</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Outdoor</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>52.645504</td>\n",
       "      <td>-1.126884</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504614</th>\n",
       "      <td>2025-06-30 01:56:10.540</td>\n",
       "      <td>0f96898a83142aa9cd14d8f27dfa1aa529af2fca</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Indoor</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>52.627893</td>\n",
       "      <td>-1.336682</td>\n",
       "      <td>-104.649658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31504615</th>\n",
       "      <td>2025-06-30 22:09:47.990</td>\n",
       "      <td>c132096ae6e1c62bf1b24eae9759b502fa51103c</td>\n",
       "      <td>None</td>\n",
       "      <td>Probably Indoor</td>\n",
       "      <td>0.5</td>\n",
       "      <td>52.662074</td>\n",
       "      <td>-1.067613</td>\n",
       "      <td>-117.042570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31504616 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp                               unique_cell  \\\n",
       "0         2025-03-01 15:48:22.455  d66c4660d6f77433b503d2e0159ce7053bdd76dc   \n",
       "1         2025-03-01 03:15:47.436  bb8abb3cfc1d2eb47edba5f216a0078d829575f2   \n",
       "2         2025-03-01 12:26:07.000  3fb387a3995398fc5602cc0e431ce935044c594e   \n",
       "3         2025-03-01 09:49:03.000  e662f0bf857466dafbb8da554abfec2e86335deb   \n",
       "4         2025-03-01 00:56:44.711  0fed16bc81fbd0ef53e6071cf9fd8449d810fdfc   \n",
       "...                           ...                                       ...   \n",
       "31504611  2025-06-30 09:21:01.666  5a6685a51c70f12cc7b03db00a18db684e60835c   \n",
       "31504612  2025-06-30 06:38:45.485  71f3829f43969786e0adb953a0390146e67d1b3b   \n",
       "31504613  2025-06-30 15:24:45.000  334868f2087c2f62b8cf9d658ba7ffec4bf9ca77   \n",
       "31504614  2025-06-30 01:56:10.540  0f96898a83142aa9cd14d8f27dfa1aa529af2fca   \n",
       "31504615  2025-06-30 22:09:47.990  c132096ae6e1c62bf1b24eae9759b502fa51103c   \n",
       "\n",
       "         measurement_type_name  in_outdoor_state  value   latitude  longitude  \\\n",
       "0                         None              None  -91.0  52.941137  -1.180972   \n",
       "1                         None   Probably Indoor   -9.0  52.756933  -1.517308   \n",
       "2                         None  Probably Outdoor -108.0  52.770351  -1.208205   \n",
       "3                         None    Surely Outdoor   46.8  52.644803  -1.189687   \n",
       "4                         None     Surely Indoor   -7.0  52.887214  -1.534917   \n",
       "...                        ...               ...    ...        ...        ...   \n",
       "31504611                  None   Probably Indoor    1.4  52.641313  -1.095236   \n",
       "31504612                  None              None -109.0  52.771422  -1.197493   \n",
       "31504613                  None  Probably Outdoor -105.0  52.645504  -1.126884   \n",
       "31504614                  None   Probably Indoor  -14.0  52.627893  -1.336682   \n",
       "31504615                  None   Probably Indoor    0.5  52.662074  -1.067613   \n",
       "\n",
       "          signal_level  \n",
       "0          -112.207920  \n",
       "1          -120.000000  \n",
       "2          -108.719592  \n",
       "3          -103.070615  \n",
       "4           -96.205530  \n",
       "...                ...  \n",
       "31504611    -86.808643  \n",
       "31504612   -102.676974  \n",
       "31504613           NaN  \n",
       "31504614   -104.649658  \n",
       "31504615   -117.042570  \n",
       "\n",
       "[31504616 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Browse the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9e6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old dataset, unused\n",
    "# df = pd.read_csv(\"hf://datasets/joefee/service-data/np_obs_jit_jf_tf_tw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9a9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin this into signal level categories\n",
    "# UK Ofcom Reference URL: \n",
    "# https://www.ofcom.org.uk/siteassets/resources/documents/phones-telecoms-and-internet/comparing-service-quality/2025/map-your-mobile-2025-threshold-methodology.pdf\n",
    "df[\"signal_level_category\"] = pd.cut(df[\"signal_level\"], bins=[-np.inf, -105, -95, -82, -74, np.inf], labels=[\"1. Very Weak\", \"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a41192b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify cells with sufficient data points\n",
    "# Cells must also have signal_level_category value\n",
    "min_points_required = 30\n",
    "\n",
    "sufficient_data_cells = duckdb.query(f\"\"\"\n",
    "WITH monthly_count AS (\n",
    "    SELECT\n",
    "        unique_cell, \n",
    "        date_trunc('month', CAST(timestamp AS timestamp)) as month, \n",
    "        COUNT(*) as count\n",
    "    FROM df\n",
    "    WHERE signal_level IS NOT NULL\n",
    "    GROUP BY unique_cell, month HAVING COUNT(*) >= {30}\n",
    ")\n",
    "SELECT unique_cell \n",
    "FROM monthly_count \n",
    "GROUP BY unique_cell\n",
    "HAVING COUNT(DISTINCT month) = {len(all_months)}\n",
    "\"\"\").to_df()['unique_cell'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0c38a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells with sufficient data: 5244\n",
      "Total number of cells: 36631\n"
     ]
    }
   ],
   "source": [
    "# Identify all unique cells\n",
    "# remove outlier cells\n",
    "outlier_cells = [\"4dc7c9ec434ed06502767136789763ec11d2c4b7\"]\n",
    "sufficient_data_cells = [cell for cell in sufficient_data_cells if cell not in outlier_cells]\n",
    "\n",
    "print(f\"Number of cells with sufficient data: {len(sufficient_data_cells)}\")\n",
    "print(f\"Total number of cells: {df['unique_cell'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d5e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_svm_boundary_geom(df, **args) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a valid MultiPolygon with true cut-out holes from One-Class SVM boundary.\n",
    "    Converts input longitude/latitude columns to float type if they are not already.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'longitude' and 'latitude' columns.\n",
    "                        These columns can contain numbers or Decimal objects.\n",
    "        **args: Keyword arguments passed directly to svm.OneClassSVM.\n",
    "\n",
    "    Returns:\n",
    "        dict: GeoJSON mapping of the resulting MultiPolygon, or None if unsuccessful.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn import svm\n",
    "    from shapely.geometry import LineString, Polygon, MultiPolygon, mapping, Point\n",
    "    from shapely.ops import polygonize, unary_union\n",
    "    import pandas as pd\n",
    "    # Import Decimal for explicit type checking/conversion if needed, though astype(float) is usually sufficient\n",
    "    from decimal import Decimal\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['longitude', 'latitude']):\n",
    "        print(\"Error: Input df must be a pandas DataFrame with 'longitude' and 'latitude' columns.\")\n",
    "        return None\n",
    "\n",
    "    if len(df) < 2:\n",
    "        print(\"Warning: Need at least 2 data points for SVM.\")\n",
    "        return None\n",
    "\n",
    "    # --- Convert coordinate columns to float type ---\n",
    "    # This resolves the Decimal vs float TypeError\n",
    "    try:\n",
    "        df_copy = df.copy() # Work on a copy to avoid modifying the original DataFrame\n",
    "        df_copy['longitude'] = df_copy['longitude'].astype(float)\n",
    "        df_copy['latitude'] = df_copy['latitude'].astype(float)\n",
    "        coords = df_copy[['longitude', 'latitude']].values\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"Error converting coordinate columns to float: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 1. Train the SVM ---\n",
    "    try:\n",
    "        clf = svm.OneClassSVM(**args)\n",
    "        clf.fit(coords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during SVM training: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. Create mesh grid for contouring ---\n",
    "    # Now calculations will use standard floats\n",
    "    x_min, x_max = coords[:, 0].min(), coords[:, 0].max()\n",
    "    y_min, y_max = coords[:, 1].min(), coords[:, 1].max()\n",
    "\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_margin = x_range * 1 if x_range > 1e-9 else 0.1\n",
    "    y_margin = y_range * 1 if y_range > 1e-9 else 0.1\n",
    "\n",
    "    x_min -= x_margin\n",
    "    x_max += x_margin\n",
    "    y_min -= y_margin\n",
    "    y_max += y_margin\n",
    "\n",
    "    resolution = 500\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n",
    "                        np.linspace(y_min, y_max, resolution))\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    try:\n",
    "        Z = clf.decision_function(grid_points).reshape(xx.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during SVM decision function evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. Extract contour lines at level 0 (the boundary) ---\n",
    "    fig, ax = plt.subplots()\n",
    "    try:\n",
    "        cs = ax.contour(xx, yy, Z, levels=[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during contour generation: {e}\")\n",
    "        plt.close(fig)\n",
    "        return None\n",
    "    plt.close(fig)\n",
    "\n",
    "    if not cs.allsegs or not cs.allsegs[0]:\n",
    "        print(\"Warning: No contour lines found at level 0.\")\n",
    "        return None\n",
    "\n",
    "    segments = cs.allsegs[0]\n",
    "    lines = [LineString(seg) for seg in segments if len(seg) >= 2]\n",
    "\n",
    "    if not lines:\n",
    "        print(\"Warning: No valid LineStrings created from contour segments.\")\n",
    "        return None\n",
    "\n",
    "    # --- 4. Polygonize the lines ---\n",
    "    try:\n",
    "        all_polygons = list(polygonize(lines))\n",
    "    except Exception as e:\n",
    "        print(f\"Error during polygonization: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not all_polygons:\n",
    "        print(\"Warning: Polygonization did not yield any polygons.\")\n",
    "        return None\n",
    "\n",
    "    # --- 5. Classify polygons and perform unary union ---\n",
    "    positive_polygons = []\n",
    "    for p in all_polygons:\n",
    "        if p.is_valid and p.area > 1e-9:\n",
    "            rep_point = p.representative_point()\n",
    "            try:\n",
    "                decision_val = clf.decision_function([[rep_point.x, rep_point.y]])[0]\n",
    "                if decision_val >= 0:\n",
    "                    # Ensure the polygon added is valid - unary_union can struggle with invalid inputs\n",
    "                    if p.is_valid:\n",
    "                        positive_polygons.append(p)\n",
    "                    else:\n",
    "                    # Attempt to buffer by 0 to fix potential self-intersections\n",
    "                        buffered_p = p.buffer(0)\n",
    "                        if buffered_p.is_valid and isinstance(buffered_p, Polygon):\n",
    "                                positive_polygons.append(buffered_p)\n",
    "                        else:\n",
    "                            print(f\"Warning: Skipping invalid polygon generated during classification step even after buffer(0). Area: {p.area}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error checking decision function for a polygon point: {e}\")\n",
    "\n",
    "\n",
    "    if not positive_polygons:\n",
    "        print(\"Warning: No valid polygons were classified as inside the SVM boundary.\")\n",
    "        return None\n",
    "\n",
    "    # --- 6. Unary Union to merge positive polygons and create holes ---\n",
    "    try:\n",
    "        # Filter again for validity just before union, as buffer(0) might create MultiPolygons\n",
    "        valid_positive_polygons = [poly for poly in positive_polygons if poly.is_valid and isinstance(poly, Polygon)]\n",
    "        if not valid_positive_polygons:\n",
    "            print(\"Warning: No valid polygons remaining before unary union.\")\n",
    "            return None\n",
    "        result_geom = unary_union(valid_positive_polygons)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch potential errors during unary_union (often related to complex topology)\n",
    "        print(f\"Error during unary union: {e}\")\n",
    "        # As a fallback, try creating a MultiPolygon directly from the valid positive polygons\n",
    "        # This might result in overlaps instead of proper union, but is better than nothing.\n",
    "        print(\"Attempting fallback: creating MultiPolygon from individual positive polygons.\")\n",
    "        try:\n",
    "            result_geom = MultiPolygon(valid_positive_polygons)\n",
    "            if not result_geom.is_valid:\n",
    "                print(\"Warning: Fallback MultiPolygon is invalid.\")\n",
    "                # Try buffer(0) on the multipolygon as a last resort\n",
    "                buffered_result = result_geom.buffer(0)\n",
    "                if buffered_result.is_valid:\n",
    "                    print(\"Fallback MultiPolygon fixed with buffer(0).\")\n",
    "                    result_geom = buffered_result\n",
    "                else:\n",
    "                    print(\"Error: Fallback MultiPolygon remains invalid even after buffer(0). Cannot proceed.\")\n",
    "                    return None\n",
    "        except Exception as fallback_e:\n",
    "            print(f\"Error during fallback MultiPolygon creation: {fallback_e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # --- 7. Format output as MultiPolygon GeoJSON mapping ---\n",
    "    final_multi_poly = None\n",
    "    if result_geom is None: # Should not happen with current logic, but check anyway\n",
    "        print(\"Error: Resulting geometry is None after union/fallback.\")\n",
    "        return None\n",
    "\n",
    "    # Simplify handling by ensuring result_geom is always iterable (list of polygons)\n",
    "    geoms_to_wrap = []\n",
    "    if isinstance(result_geom, Polygon):\n",
    "        if result_geom.is_valid:\n",
    "            geoms_to_wrap = [result_geom]\n",
    "    elif isinstance(result_geom, MultiPolygon):\n",
    "        # Filter out invalid geoms within the MultiPolygon if any\n",
    "        geoms_to_wrap = [g for g in result_geom.geoms if g.is_valid and isinstance(g, Polygon)]\n",
    "    elif hasattr(result_geom, 'geoms'): # Handle GeometryCollection\n",
    "        print(f\"Warning: unary_union resulted in a GeometryCollection. Filtering for valid Polygons.\")\n",
    "        geoms_to_wrap = [g for g in result_geom.geoms if g.is_valid and isinstance(g, Polygon)]\n",
    "\n",
    "    if not geoms_to_wrap:\n",
    "        print(\"Warning: No valid polygons found in the final geometry after union/cleanup.\")\n",
    "        return None\n",
    "\n",
    "    # Create the final MultiPolygon\n",
    "    final_multi_poly = MultiPolygon(geoms_to_wrap)\n",
    "\n",
    "    # Final validity check\n",
    "    if final_multi_poly.is_valid:\n",
    "        return mapping(final_multi_poly)\n",
    "    else:\n",
    "        # Try one last buffer(0) fix\n",
    "        print(\"Warning: Final MultiPolygon is invalid. Attempting buffer(0) fix.\")\n",
    "        buffered_final = final_multi_poly.buffer(0)\n",
    "        if buffered_final.is_valid and isinstance(buffered_final, (Polygon, MultiPolygon)):\n",
    "            # Re-wrap if buffer resulted in a single Polygon\n",
    "            if isinstance(buffered_final, Polygon):\n",
    "                final_multi_poly = MultiPolygon([buffered_final])\n",
    "            else:\n",
    "                final_multi_poly = buffered_final\n",
    "            print(\"Final MultiPolygon fixed with buffer(0).\")\n",
    "            return mapping(final_multi_poly)\n",
    "        else:\n",
    "            print(\"Error: Final MultiPolygon remains invalid even after buffer(0).\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b381efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_cells = [\"5a6685a51c70f12cc7b03db00a18db684e60835c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ebad8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Processing cell 5a6685a51c70f12cc7b03db00a18db684e60835c\n",
      "---Training month: 2025-03-01, Testing month: 2025-04-30---\n",
      "Train: 3520, Test: 3548\n",
      "Convex Hull: 3344 filtered points, 3424 out of 3548 test points within hull\n",
      "New best convex hull based on recall: 0.9651\n",
      "New best SVM hyperplane for levels ['5. Very Strong'] based on recall: -1 -> 0.9077\n",
      "New best SVM hyperplane for levels ['4. Strong', '5. Very Strong'] based on recall: -1 -> 0.8922\n",
      "New best SVM hyperplane for levels ['3. Moderate', '4. Strong', '5. Very Strong'] based on recall: -1 -> 0.9309\n",
      "New best SVM hyperplane for levels ['2. Weak', '3. Moderate', '4. Strong', '5. Very Strong'] based on recall: -1 -> 0.9394\n",
      "New best SVM hyperplane for levels ['1. Very Weak', '2. Weak', '3. Moderate', '4. Strong', '5. Very Strong'] based on recall: -1 -> 0.9392\n",
      "---Training month: 2025-04-01, Testing month: 2025-05-31---\n",
      "Train: 3548, Test: 7015\n",
      "Convex Hull: 3370 filtered points, 6630 out of 7015 test points within hull\n",
      "Skip. SVM hyperplane for levels ['5. Very Strong'] did not improve based on recall: 0.8559 (best: 0.9077)\n",
      "New best SVM hyperplane for levels ['4. Strong', '5. Very Strong'] based on recall: 0.8921568627450981 -> 0.9288\n",
      "New best SVM hyperplane for levels ['3. Moderate', '4. Strong', '5. Very Strong'] based on recall: 0.9308641975308642 -> 0.9311\n",
      "Skip. SVM hyperplane for levels ['2. Weak', '3. Moderate', '4. Strong', '5. Very Strong'] did not improve based on recall: 0.9285 (best: 0.9394)\n",
      "Skip. SVM hyperplane for levels ['1. Very Weak', '2. Weak', '3. Moderate', '4. Strong', '5. Very Strong'] did not improve based on recall: 0.9296 (best: 0.9392)\n",
      "---Training month: 2025-05-01, Testing month: 2025-06-30---\n",
      "Train: 7015, Test: 6926\n",
      "Convex Hull: 6664 filtered points, 6151 out of 6926 test points within hull\n",
      "Skip. SVM hyperplane for levels ['5. Very Strong'] did not improve based on recall: 0.8160 (best: 0.9077)\n",
      "Skip. SVM hyperplane for levels ['4. Strong', '5. Very Strong'] did not improve based on recall: 0.8046 (best: 0.9288)\n",
      "Skip. SVM hyperplane for levels ['3. Moderate', '4. Strong', '5. Very Strong'] did not improve based on recall: 0.8098 (best: 0.9311)\n",
      "Skip. SVM hyperplane for levels ['2. Weak', '3. Moderate', '4. Strong', '5. Very Strong'] did not improve based on recall: 0.8263 (best: 0.9394)\n",
      "Skip. SVM hyperplane for levels ['1. Very Weak', '2. Weak', '3. Moderate', '4. Strong', '5. Very Strong'] did not improve based on recall: 0.8112 (best: 0.9392)\n"
     ]
    }
   ],
   "source": [
    "metric = \"recall\"  # Options: \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "\n",
    "for cell_id in [x for x in sufficient_data_cells if x in allowed_cells]:\n",
    "\n",
    "    my_geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "    }\n",
    "\n",
    "    my_best_convex_hull:tuple[float, BaseGeometry] = None\n",
    "    my_best_svm_hyperplanes:Dict[Any, tuple[float, BaseGeometry]] = {}\n",
    "\n",
    "    # Select data for this cell only\n",
    "    df_cell = df[df['unique_cell'] == cell_id]\n",
    "    print(f\"---\")\n",
    "    print(f\"Processing cell {cell_id}\")\n",
    "\n",
    "    # Exclude last month\n",
    "    for month in all_months[:-1]:\n",
    "        \n",
    "        # Random colour for each month\n",
    "        colour = \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "\n",
    "        # Identify train/test split\n",
    "        # Use one month for training, next month for testing\n",
    "        df_train = df_cell[(pd.to_datetime(df_cell['timestamp']).dt.to_period('M') == pd.to_datetime(month).to_period('M'))]\n",
    "        test_month = (pd.to_datetime(month).to_period('M') + 1).strftime(\"%Y-%m-%d\")\n",
    "        df_test = df_cell[(pd.to_datetime(df_cell['timestamp']).dt.to_period('M') == pd.to_datetime(test_month).to_period('M'))]\n",
    "\n",
    "        print(f\"---Training month: {month}, Testing month: {test_month}---\")\n",
    "        print(f\"Train: {len(df_train)}, Test: {len(df_test)}\")\n",
    "\n",
    "        # Construct a convex hull with shapely using train_df\n",
    "        points_train = [Point(xy) for xy in zip(df_train['longitude'], df_train['latitude'])]\n",
    "        points_test = [Point(xy) for xy in zip(df_test['longitude'], df_test['latitude'])]\n",
    "\n",
    "        # Find center of mass among these points\n",
    "        multipoint = MultiPoint(points_train)\n",
    "        center_of_mass = multipoint.centroid\n",
    "\n",
    "        # For each point, calculate distance to center of mass\n",
    "        distances = [point.distance(center_of_mass) for point in points_train]\n",
    "        \n",
    "        # Find the 95% percentile distance\n",
    "        threshold_distance = pd.Series(distances).quantile(0.95)\n",
    "\n",
    "        # Filter points to only those within the threshold distance\n",
    "        filtered_points = [point for point, distance in zip(points_train, distances) if distance <= threshold_distance]\n",
    "\n",
    "        # Create new multipoint from filtered points\n",
    "        multipoint_filtered = MultiPoint(filtered_points)\n",
    "        \n",
    "        # Calculate the convex hull\n",
    "        convex_hull = multipoint_filtered.convex_hull\n",
    "\n",
    "        # Validate against test_df\n",
    "        # Count how many test points fall within the convex hull\n",
    "        test_points_within = [point for point in points_test if convex_hull.contains(point)]\n",
    "        print(f\"Convex Hull: {len(filtered_points)} filtered points, {len(test_points_within)} out of {len(points_test)} test points within hull\")\n",
    "\n",
    "        # Construct confusion matrix values\n",
    "        y_pred = [True if convex_hull.contains(point) else False for point in points_test]\n",
    "        y_true = [True] * len(points_test)\n",
    "\n",
    "        # Compute classification metrics\n",
    "        accuracy_score_value = accuracy_score(y_true, y_pred)\n",
    "        precision_score_value = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall_score_value = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1_score_value = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        # Add best convex hull based on selected metric\n",
    "        current_metric_value = None\n",
    "        if metric == \"accuracy\":\n",
    "            current_metric_value = accuracy_score_value\n",
    "        elif metric == \"precision\":\n",
    "            current_metric_value = precision_score_value\n",
    "        elif metric == \"recall\":\n",
    "            current_metric_value = recall_score_value\n",
    "        elif metric == \"f1\":\n",
    "            current_metric_value = f1_score_value\n",
    "        else:\n",
    "            print(f\"Warning: Unknown metric '{metric}' specified for best convex hull selection.\")\n",
    "        \n",
    "        if my_best_convex_hull is None or (current_metric_value is not None and current_metric_value > my_best_convex_hull[0]):\n",
    "            my_best_convex_hull = (current_metric_value, convex_hull)\n",
    "            print(f\"New best convex hull based on {metric}: {current_metric_value:.4f}\")\n",
    "        \n",
    "\n",
    "        # Now generate SVM boundary using filtered points\n",
    "        svm_args = {\n",
    "            'kernel': 'rbf',\n",
    "            'nu': float(0.08),  # Lower value allow more flexible boundary\n",
    "            'gamma': float(len(df_train) * 30)  # Higher value allow more complex decision boundary\n",
    "        }\n",
    "\n",
    "        # Run SVM for different signal level categories\n",
    "        # Starting from very weak to very strong\n",
    "        # Cumulative union of all these geoms to ensure coverage of all signal levels\n",
    "        levels = {\n",
    "            5: [\"5. Very Strong\"],\n",
    "            4: [\"4. Strong\", \"5. Very Strong\"],\n",
    "            3: [\"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "            2: [\"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "            1: [\"1. Very Weak\", \"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "        }\n",
    "        \n",
    "        # Generate SVM boundaries for each level\n",
    "        for key, level in levels.items():\n",
    "            df_subset_level_train = df_train[df_train[\"signal_level_category\"].isin(level)]\n",
    "\n",
    "            # Use all levels for testing\n",
    "            df_subset_level_test = df_test\n",
    "\n",
    "            if len(df_subset_level_train) < min_points_required:\n",
    "                print(f\"Skipping SVM for cell {cell_id} at levels {level} due to insufficient points ({len(df_subset_level_train)} < {min_points_required})\")\n",
    "                continue\n",
    "            \n",
    "            points_level_train = [Point(xy) for xy in zip(df_subset_level_train['longitude'], df_subset_level_train['latitude'])]\n",
    "            points_level_test = [Point(xy) for xy in zip(df_subset_level_test['longitude'], df_subset_level_test['latitude'])]\n",
    "            \n",
    "            # Run SVM boundary generation\n",
    "            buffer = 0.000 \n",
    "            geom_level = generate_svm_boundary_geom(df_subset_level_train, **svm_args)\n",
    "            \n",
    "            if geom_level is not None:\n",
    "                # If the function returned a GeoJSON mapping (dict), convert to shapely geometry\n",
    "                if isinstance(geom_level, dict):\n",
    "                    try:\n",
    "                        geom_obj = shape(geom_level)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: could not convert generated mapping to shapely geometry for cell {cell_id}, levels {level}: {e}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    geom_obj = geom_level\n",
    "\n",
    "                # Add buffer radius to have better coverage\n",
    "                if buffer > 0:\n",
    "                    try:\n",
    "                        geom_obj = geom_obj.buffer(buffer)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: buffering geometry failed for cell {cell_id}, levels {level}: {e}\")\n",
    "\n",
    "                # Ensure the geometry is serialized to GeoJSON mapping (dict) before appending\n",
    "                try:\n",
    "                    geom_mapping = shapely.geometry.mapping(geom_obj)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: could not create mapping for geometry for cell {cell_id}, levels {level}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Construct confusion matrix values\n",
    "                y_pred = [True if geom_obj.contains(point) else False for point in points_level_test]\n",
    "                y_true = [x in level for x in df_subset_level_test[\"signal_level_category\"].tolist()]\n",
    "\n",
    "                # Compute classification metrics\n",
    "                accuracy_score_value = accuracy_score(y_true, y_pred)\n",
    "                precision_score_value = precision_score(y_true, y_pred, zero_division=0)\n",
    "                recall_score_value = recall_score(y_true, y_pred, zero_division=0)\n",
    "                f1_score_value = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "                # Store best SVM hyperplane based on selected metric\n",
    "                current_metric_value = None\n",
    "                if metric == \"accuracy\":\n",
    "                    current_metric_value = accuracy_score_value\n",
    "                elif metric == \"precision\":\n",
    "                    current_metric_value = precision_score_value\n",
    "                elif metric == \"recall\":\n",
    "                    current_metric_value = recall_score_value\n",
    "                elif metric == \"f1\":\n",
    "                    current_metric_value = f1_score_value\n",
    "                else:\n",
    "                    print(f\"Warning: Unknown metric '{metric}' specified for best SVM hyperplane selection.\")\n",
    "\n",
    "                if key not in my_best_svm_hyperplanes or (current_metric_value is not None and current_metric_value > my_best_svm_hyperplanes[key][0]):\n",
    "                    old_metric_value = my_best_svm_hyperplanes[key][0] if key in my_best_svm_hyperplanes else -1\n",
    "                    print(f\"New best SVM hyperplane for levels {level} based on {metric}: {old_metric_value} -> {current_metric_value:.4f}\")\n",
    "                    my_best_svm_hyperplanes[key] = (current_metric_value, geom_obj)\n",
    "                else:\n",
    "                    print(f\"Skip. SVM hyperplane for levels {level} did not improve based on {metric}: {current_metric_value:.4f} (best: {my_best_svm_hyperplanes[key][0]:.4f})\")\n",
    "                \n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(f\"Warning: SVM boundary generation failed for cell {cell_id} at levels {level}\")\n",
    "        \n",
    "        \n",
    "    # Append best convex hull to list\n",
    "    my_geojson['features'].append({\n",
    "        \"type\": \"Feature\",\n",
    "        \"properties\": {\n",
    "            \"cell_id\": cell_id,\n",
    "            \"fill\": \"#3300FF\",\n",
    "        },\n",
    "        \"geometry\": shapely.geometry.mapping(my_best_convex_hull[1])\n",
    "    })\n",
    "\n",
    "    # Ensure successive hyperplanes are fully overlapping\n",
    "    # For example, level 5 should be fully within level 4, and level 4 within level 3, etc.\n",
    "    # This means level 4 is the union of level 4 and level 5, level 3 is the union of level 3, level 4 and level 5, etc.\n",
    "    for i, key in enumerate(sorted(my_best_svm_hyperplanes.keys(), reverse=True)):\n",
    "        if i == 0:\n",
    "            continue  # Skip the highest level as there's nothing above it\n",
    "        higher_key = sorted(my_best_svm_hyperplanes.keys(), reverse=True)[i - 1]\n",
    "        if higher_key in my_best_svm_hyperplanes:\n",
    "            higher_geom = my_best_svm_hyperplanes[higher_key][1]\n",
    "            current_geom = my_best_svm_hyperplanes[key][1]\n",
    "            # Perform union to ensure full coverage\n",
    "            try:\n",
    "                combined_geom = higher_geom.union(current_geom)\n",
    "                my_best_svm_hyperplanes[key] = (my_best_svm_hyperplanes[key][0], combined_geom)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not union geometries for levels {higher_key} and {key} for cell {cell_id}: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Append the best SVM hyperplanes to list\n",
    "    for key, value in my_best_svm_hyperplanes.items():\n",
    "        my_geojson['features'].append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"cell_id\": cell_id,\n",
    "                \"svm_boundary_levels\": key,\n",
    "                \"fill\": \"#FF0000\",\n",
    "            },\n",
    "            \"geometry\": shapely.geometry.mapping(value[1]),\n",
    "        })\n",
    "        \n",
    "    # Export geojson for this cell\n",
    "    import os\n",
    "    os.makedirs('cells', exist_ok=True)\n",
    "    with open(f\"cells/cell_{cell_id}.geojson\", \"w\") as f:\n",
    "        json.dump(my_geojson, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile-coverage-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
