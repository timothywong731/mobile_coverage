{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd91c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "import duckdb\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Optional\n",
    "from shapely.geometry import shape\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/joefee/service-data/np_obs_jit_jf_tf_tw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin this into signal level categories\n",
    "# UK Ofcom Reference URL: \n",
    "# https://www.ofcom.org.uk/siteassets/resources/documents/phones-telecoms-and-internet/comparing-service-quality/2025/map-your-mobile-2025-threshold-methodology.pdf\n",
    "df[\"signal_level_category\"] = pd.cut(df[\"signal_level\"], bins=[-np.inf, -105, -95, -82, -74, np.inf], labels=[\"1. Very Weak\", \"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41192b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify cells with sufficient data points\n",
    "# Cells must also have signal_level_category value\n",
    "min_points_required = 30\n",
    "\n",
    "sufficient_data_cells = duckdb.query(f\"\"\"\n",
    "    SELECT unique_cell \n",
    "    FROM df\n",
    "    WHERE signal_level_category IS NOT NULL\n",
    "    GROUP BY unique_cell HAVING COUNT(*) >= {min_points_required}\n",
    "\"\"\").to_df()['unique_cell'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all unique cells\n",
    "# remove outlier cells\n",
    "outlier_cells = [\"4dc7c9ec434ed06502767136789763ec11d2c4b7\"]\n",
    "sufficient_data_cells = [cell for cell in sufficient_data_cells if cell not in outlier_cells]\n",
    "\n",
    "print(f\"Number of cells with sufficient data: {len(sufficient_data_cells)}\")\n",
    "print(f\"Total number of cells: {df['unique_cell'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_svm_boundary_geom(df, **args) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a valid MultiPolygon with true cut-out holes from One-Class SVM boundary.\n",
    "    Converts input longitude/latitude columns to float type if they are not already.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'longitude' and 'latitude' columns.\n",
    "                        These columns can contain numbers or Decimal objects.\n",
    "        **args: Keyword arguments passed directly to svm.OneClassSVM.\n",
    "\n",
    "    Returns:\n",
    "        dict: GeoJSON mapping of the resulting MultiPolygon, or None if unsuccessful.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn import svm\n",
    "    from shapely.geometry import LineString, Polygon, MultiPolygon, mapping, Point\n",
    "    from shapely.ops import polygonize, unary_union\n",
    "    import pandas as pd\n",
    "    # Import Decimal for explicit type checking/conversion if needed, though astype(float) is usually sufficient\n",
    "    from decimal import Decimal\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame) or not all(col in df.columns for col in ['longitude', 'latitude']):\n",
    "        print(\"Error: Input df must be a pandas DataFrame with 'longitude' and 'latitude' columns.\")\n",
    "        return None\n",
    "\n",
    "    if len(df) < 2:\n",
    "        print(\"Warning: Need at least 2 data points for SVM.\")\n",
    "        return None\n",
    "\n",
    "    # --- Convert coordinate columns to float type ---\n",
    "    # This resolves the Decimal vs float TypeError\n",
    "    try:\n",
    "        df_copy = df.copy() # Work on a copy to avoid modifying the original DataFrame\n",
    "        df_copy['longitude'] = df_copy['longitude'].astype(float)\n",
    "        df_copy['latitude'] = df_copy['latitude'].astype(float)\n",
    "        coords = df_copy[['longitude', 'latitude']].values\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"Error converting coordinate columns to float: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 1. Train the SVM ---\n",
    "    try:\n",
    "        clf = svm.OneClassSVM(**args)\n",
    "        clf.fit(coords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during SVM training: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. Create mesh grid for contouring ---\n",
    "    # Now calculations will use standard floats\n",
    "    x_min, x_max = coords[:, 0].min(), coords[:, 0].max()\n",
    "    y_min, y_max = coords[:, 1].min(), coords[:, 1].max()\n",
    "\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_margin = x_range * 1 if x_range > 1e-9 else 0.1\n",
    "    y_margin = y_range * 1 if y_range > 1e-9 else 0.1\n",
    "\n",
    "    x_min -= x_margin\n",
    "    x_max += x_margin\n",
    "    y_min -= y_margin\n",
    "    y_max += y_margin\n",
    "\n",
    "    resolution = 500\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n",
    "                        np.linspace(y_min, y_max, resolution))\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    try:\n",
    "        Z = clf.decision_function(grid_points).reshape(xx.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during SVM decision function evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. Extract contour lines at level 0 (the boundary) ---\n",
    "    fig, ax = plt.subplots()\n",
    "    try:\n",
    "        cs = ax.contour(xx, yy, Z, levels=[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during contour generation: {e}\")\n",
    "        plt.close(fig)\n",
    "        return None\n",
    "    plt.close(fig)\n",
    "\n",
    "    if not cs.allsegs or not cs.allsegs[0]:\n",
    "        print(\"Warning: No contour lines found at level 0.\")\n",
    "        return None\n",
    "\n",
    "    segments = cs.allsegs[0]\n",
    "    lines = [LineString(seg) for seg in segments if len(seg) >= 2]\n",
    "\n",
    "    if not lines:\n",
    "        print(\"Warning: No valid LineStrings created from contour segments.\")\n",
    "        return None\n",
    "\n",
    "    # --- 4. Polygonize the lines ---\n",
    "    try:\n",
    "        all_polygons = list(polygonize(lines))\n",
    "    except Exception as e:\n",
    "        print(f\"Error during polygonization: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not all_polygons:\n",
    "        print(\"Warning: Polygonization did not yield any polygons.\")\n",
    "        return None\n",
    "\n",
    "    # --- 5. Classify polygons and perform unary union ---\n",
    "    positive_polygons = []\n",
    "    for p in all_polygons:\n",
    "        if p.is_valid and p.area > 1e-9:\n",
    "            rep_point = p.representative_point()\n",
    "            try:\n",
    "                decision_val = clf.decision_function([[rep_point.x, rep_point.y]])[0]\n",
    "                if decision_val >= 0:\n",
    "                    # Ensure the polygon added is valid - unary_union can struggle with invalid inputs\n",
    "                    if p.is_valid:\n",
    "                        positive_polygons.append(p)\n",
    "                    else:\n",
    "                    # Attempt to buffer by 0 to fix potential self-intersections\n",
    "                        buffered_p = p.buffer(0)\n",
    "                        if buffered_p.is_valid and isinstance(buffered_p, Polygon):\n",
    "                                positive_polygons.append(buffered_p)\n",
    "                        else:\n",
    "                            print(f\"Warning: Skipping invalid polygon generated during classification step even after buffer(0). Area: {p.area}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error checking decision function for a polygon point: {e}\")\n",
    "\n",
    "\n",
    "    if not positive_polygons:\n",
    "        print(\"Warning: No valid polygons were classified as inside the SVM boundary.\")\n",
    "        return None\n",
    "\n",
    "    # --- 6. Unary Union to merge positive polygons and create holes ---\n",
    "    try:\n",
    "        # Filter again for validity just before union, as buffer(0) might create MultiPolygons\n",
    "        valid_positive_polygons = [poly for poly in positive_polygons if poly.is_valid and isinstance(poly, Polygon)]\n",
    "        if not valid_positive_polygons:\n",
    "            print(\"Warning: No valid polygons remaining before unary union.\")\n",
    "            return None\n",
    "        result_geom = unary_union(valid_positive_polygons)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch potential errors during unary_union (often related to complex topology)\n",
    "        print(f\"Error during unary union: {e}\")\n",
    "        # As a fallback, try creating a MultiPolygon directly from the valid positive polygons\n",
    "        # This might result in overlaps instead of proper union, but is better than nothing.\n",
    "        print(\"Attempting fallback: creating MultiPolygon from individual positive polygons.\")\n",
    "        try:\n",
    "            result_geom = MultiPolygon(valid_positive_polygons)\n",
    "            if not result_geom.is_valid:\n",
    "                print(\"Warning: Fallback MultiPolygon is invalid.\")\n",
    "                # Try buffer(0) on the multipolygon as a last resort\n",
    "                buffered_result = result_geom.buffer(0)\n",
    "                if buffered_result.is_valid:\n",
    "                    print(\"Fallback MultiPolygon fixed with buffer(0).\")\n",
    "                    result_geom = buffered_result\n",
    "                else:\n",
    "                    print(\"Error: Fallback MultiPolygon remains invalid even after buffer(0). Cannot proceed.\")\n",
    "                    return None\n",
    "        except Exception as fallback_e:\n",
    "            print(f\"Error during fallback MultiPolygon creation: {fallback_e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # --- 7. Format output as MultiPolygon GeoJSON mapping ---\n",
    "    final_multi_poly = None\n",
    "    if result_geom is None: # Should not happen with current logic, but check anyway\n",
    "        print(\"Error: Resulting geometry is None after union/fallback.\")\n",
    "        return None\n",
    "\n",
    "    # Simplify handling by ensuring result_geom is always iterable (list of polygons)\n",
    "    geoms_to_wrap = []\n",
    "    if isinstance(result_geom, Polygon):\n",
    "        if result_geom.is_valid:\n",
    "            geoms_to_wrap = [result_geom]\n",
    "    elif isinstance(result_geom, MultiPolygon):\n",
    "        # Filter out invalid geoms within the MultiPolygon if any\n",
    "        geoms_to_wrap = [g for g in result_geom.geoms if g.is_valid and isinstance(g, Polygon)]\n",
    "    elif hasattr(result_geom, 'geoms'): # Handle GeometryCollection\n",
    "        print(f\"Warning: unary_union resulted in a GeometryCollection. Filtering for valid Polygons.\")\n",
    "        geoms_to_wrap = [g for g in result_geom.geoms if g.is_valid and isinstance(g, Polygon)]\n",
    "\n",
    "    if not geoms_to_wrap:\n",
    "        print(\"Warning: No valid polygons found in the final geometry after union/cleanup.\")\n",
    "        return None\n",
    "\n",
    "    # Create the final MultiPolygon\n",
    "    final_multi_poly = MultiPolygon(geoms_to_wrap)\n",
    "\n",
    "    # Final validity check\n",
    "    if final_multi_poly.is_valid:\n",
    "        return mapping(final_multi_poly)\n",
    "    else:\n",
    "        # Try one last buffer(0) fix\n",
    "        print(\"Warning: Final MultiPolygon is invalid. Attempting buffer(0) fix.\")\n",
    "        buffered_final = final_multi_poly.buffer(0)\n",
    "        if buffered_final.is_valid and isinstance(buffered_final, (Polygon, MultiPolygon)):\n",
    "            # Re-wrap if buffer resulted in a single Polygon\n",
    "            if isinstance(buffered_final, Polygon):\n",
    "                final_multi_poly = MultiPolygon([buffered_final])\n",
    "            else:\n",
    "                final_multi_poly = buffered_final\n",
    "            print(\"Final MultiPolygon fixed with buffer(0).\")\n",
    "            return mapping(final_multi_poly)\n",
    "        else:\n",
    "            print(\"Error: Final MultiPolygon remains invalid even after buffer(0).\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing loop for each cell\n",
    "buffer = 0.0\n",
    "\n",
    "\n",
    "for cell_id in sufficient_data_cells:\n",
    "\n",
    "    # Initialize empty geojson for this cell\n",
    "    my_geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "    }\n",
    "    # Subset the dataframe to only this cell\n",
    "    df_subset = df[df['unique_cell'] == cell_id]\n",
    "\n",
    "    # Skip if not enough points\n",
    "    if len(df_subset) < min_points_required:\n",
    "        print(f\"Skipping cell {cell_id} due to insufficient points ({len(df_subset)} < {min_points_required})\")\n",
    "        continue\n",
    "\n",
    "    # Get indoors data excluding \"Full Service Loss (>120s)\"\n",
    "    df_indoors = df_subset[ (df_subset[\"in_outdoor_state\"].isin([\"Probably Indoor\", \"Surely Indoor\"])) & (df_subset['measurement_type_name'] != \"Full Service Loss (>120s)\") ]\n",
    "    df_outdoors = df_subset[ (df_subset[\"in_outdoor_state\"].isin([\"Probably Outdoor\", \"Surely Outdoor\"])) & (df_subset['measurement_type_name'] != \"Full Service Loss (>120s)\") ]\n",
    "\n",
    "    print(f\"---\")\n",
    "    print(f\"Cell ID: {cell_id}, Total Records: {len(df_subset)}\")\n",
    "    print(f\"Indoor Records (excluding Full Service Loss): {len(df_indoors)}\")\n",
    "    print(f\"Outdoor Records (excluding Full Service Loss): {len(df_outdoors)}\")\n",
    "\n",
    "    # Construct a convex hull with shapely using df_subset\n",
    "    points = [Point(xy) for xy in zip(df_subset['longitude'], df_subset['latitude'])]\n",
    "\n",
    "    # Find center of mass among these points\n",
    "    multipoint = MultiPoint(points)\n",
    "    center_of_mass = multipoint.centroid\n",
    "\n",
    "    # For each point, calculate distance to center of mass\n",
    "    distances = [point.distance(center_of_mass) for point in points]\n",
    "    \n",
    "    # Find the 95% percentile distance\n",
    "    threshold_distance = pd.Series(distances).quantile(0.95)\n",
    "\n",
    "    # Filter points to only those within the threshold distance\n",
    "    filtered_points = [point for point, distance in zip(points, distances) if distance <= threshold_distance]\n",
    "\n",
    "    # Create new multipoint from filtered points\n",
    "    multipoint_filtered = MultiPoint(filtered_points)\n",
    "    \n",
    "    # Calculate the convex hull\n",
    "    convex_hull = multipoint_filtered.convex_hull\n",
    "\n",
    "    # Append convex hull to list\n",
    "    # my_geojson['features'].append({\n",
    "    #     \"type\": \"Feature\",\n",
    "    #     \"properties\": {\n",
    "    #         \"cell_id\": cell_id,\n",
    "    #         \"total_records\": len(df_subset),\n",
    "    #         \"fill\": \"#3300FF\",\n",
    "    #     },\n",
    "    #     \"geometry\": shapely.geometry.mapping(convex_hull)\n",
    "    # })\n",
    "\n",
    "    # Now generate SVM boundary using filtered points\n",
    "    svm_args = {\n",
    "        'kernel': 'rbf',\n",
    "        'nu': float(0.08),  # Lower value allow more flexible boundary\n",
    "        'gamma': float(len(df_subset) * 50)  # Higher value allow more complex decision boundary\n",
    "    }\n",
    "\n",
    "    # Run SVM for different signal level categories\n",
    "    # Starting from verry weak to very strong\n",
    "    # Cumulative union of all these geoms to ensure coverage of all signal levels\n",
    "    levels = [\n",
    "        [\"5. Very Strong\"],\n",
    "        [\"4. Strong\", \"5. Very Strong\"],\n",
    "        [\"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "        [\"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "        [\"1. Very Weak\", \"2. Weak\", \"3. Moderate\", \"4. Strong\", \"5. Very Strong\"],\n",
    "    ]\n",
    "\n",
    "    # Generate SVM boundaries for each level\n",
    "    for level in levels:\n",
    "        df_subset_level = df_subset[df_subset['signal_level_category'].isin(level)]\n",
    "        if len(df_subset_level) < min_points_required:\n",
    "            print(f\"Skipping SVM for cell {cell_id} at levels {level} due to insufficient points ({len(df_subset_level)} < {min_points_required})\")\n",
    "            continue\n",
    "\n",
    "        geom_level = generate_svm_boundary_geom(df_subset_level, **svm_args)\n",
    "        \n",
    "        if geom_level is not None:\n",
    "            # Add buffer radius to have better coverage\n",
    "\n",
    "            if buffer > 0:\n",
    "                geom_level = shape(geom_level).buffer(buffer)\n",
    "\n",
    "            # Ensure the geometry is serialized to GeoJSON mapping (dict) before appending\n",
    "            my_geojson['features'].append({\n",
    "                \"type\": \"Feature\",\n",
    "                \"properties\": {\n",
    "                    \"cell_id\": cell_id,\n",
    "                    \"total_records\": len(df_subset_level),\n",
    "                    \"svm_boundary_levels\": level,\n",
    "                    \"fill\": \"#00FF00\",\n",
    "                },\n",
    "                \"geometry\": shapely.geometry.mapping(geom_level),\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: SVM boundary generation failed for cell {cell_id} at levels {level}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # Add points to the export\n",
    "    for i, point in enumerate(points):\n",
    "        my_geojson['features'].append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"cell_id\": cell_id,\n",
    "                \"point_index\": i,\n",
    "                \"marker-color\": \"#FF0000\",\n",
    "                \"marker-symbol\": \"circle\",\n",
    "            },\n",
    "            \"geometry\": shapely.geometry.mapping(point)\n",
    "        })\n",
    "    \n",
    "    # Export geojson \n",
    "    with open(f\"cells/cell_{cell_id}.geojson\", \"w\") as f:\n",
    "        json.dump(my_geojson, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile-coverage-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
